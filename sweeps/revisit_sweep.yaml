program: src/REVISIT.py
method: random  # Options: grid, random, bayes (random recommended for large search spaces)
run_cap: null  # Maximum number of runs (null for unlimited, set to number to limit)
metric:
  name: val_loss  # Use validation loss to guide sweep (minimize to avoid overfitting)
  goal: minimize
  # Alternative: use train_loss if no validation set is provided
  # name: train_loss
  # goal: minimize

parameters:
  input:
    value: train_2m.csv  # Update with your actual training file path
  
  k:
    values: [4]  # k-mer sizes to explore
  
  dim:
    values: [128, 256, 512]  # Embedding dimensions to explore
  
  lr:
    values: [0.00005, 0.0001, 0.0002, 0.0005]  # Learning rates to explore
  
  epoch:
    values: [5, 10, 15, 20]  # Number of epochs to explore
  
  batch_size:
    values: [16, 32, 64, 128]  # Batch sizes to explore (0 means full dataset)
  
  neg_sample_per_pos:
    values: [50, 100, 200]  # Negative samples per positive to explore
  
  loss_name:
    values: ["bern"]  # Loss function (can add more if supported)
  
  max_read_num:
    values: [50, 100, 200]  # Maximum reads to use (0 means all)
  
  seed:
    value: 26042024
  
  device:
    value: cuda
  
  workers_num:
    value: 1
  
  checkpoint:
    value: 0
  
  output:
    value: models/revisit_model.pt  # Will be saved with epoch and LR in filename by the script
  
  wandb_project:
    value: dna-embedding-revisit
  
  wandb_mode:
    value: online
  
  val_input:
    value: val_48k.csv  # Validation file for computing validation loss (used for early stopping and sweep optimization)
  
  # Evaluation disabled during sweep - use separate evaluation script
  # eval_data_dir:
  #   value: null

